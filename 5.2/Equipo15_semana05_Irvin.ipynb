{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"m32FOTlSU6D_"},"source":["# **Maestría en Inteligencia Artificial Aplicada**\n","## **Curso: Procesamiento de Lenguaje Natural (NLP)**\n","### Tecnológico de Monterrey\n","### Prof Luis Eduardo Falcón Morales\n","\n","## **Adtividad Semanas 05**\n","\n","### **Modelos Embebidos: Caso Amazon/Yelp/IMDb**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"g-gSZZXBtEsq"},"source":["#### Equipo 15\n","\n","**Nombres y matrículas de los integrantes del equipo:**\n","\n","* Javier de Alba Pérez $\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,$ A01226046\n","* Irvin Gomez Esquivel $\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,$ A00361034\n","* Daniel Camacho $\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,$ A01793555\n","* Manuel Gerardo Licera Aguirre $\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,$ A00947315\n","* Andrés Eduardo Figueroa García $\\,\\,\\,\\,\\,\\,$ A01378536"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tHcYbYtctEsq"},"source":["## **Instrucciones**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xfwc0y44VZfg"},"source":["En la actividad de esta semana trabajarás en equipos con el modelo de vectores continuos/embebidos FastText,\n","es decir, el modelo desarrollado por Facebook en 2016.\n","\n","Una manera de trabajar con estos modelos pre-entrenados, es generando el vocabulario a partir de tu conjunto\n","de datos de entrenamiento. Para cada palabra de tu vocabulario, podrás sustituirlo por su correspondiente\n","vector continuo. En caso de que no exista el vector para una palabra en particular, se puede eliminar dicha\n","palabra, o bien sustituirla por el vector continuo más cercano. En esta actividad deberás aplicar esta segunda\n","opción. Existen diversas propuestas para utilizar dichos vectores continuos como entrada para modelos de\n","aprendizaje automático. En particular, en esta actividad cada enunciado será sustituido por el vector promedio\n","de todos los tokens que lo forman."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"f-8CSIWitEsr"},"source":["## **Librerias**"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3183,"status":"ok","timestamp":1684698557712,"user":{"displayName":"Manuel Gerardo","userId":"00401288527964521163"},"user_tz":360},"id":"wCL2p6MA8NuT"},"outputs":[],"source":["import pandas as pd  \n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","import re\n","import string"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4c34ZOnna3Gu"},"source":["## **Pregunta - 1:**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yeNllxRdmeWg"},"source":["Descarga los 3 archivos de Canvas. En particular, el archivo de datos de IMDb ya no requiere\n","transformarse para obtener sus 1000 registros. Al cargar los datos de los tres archivos deberás\n","tener un DataFrame de Pandas de 3000 registros, con sus etiquetas. Los archivos los encuentras en\n","Canvas y se llaman: amazon5.txt, imdb5.txt, yelp5.txt."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122,"status":"ok","timestamp":1684698559839,"user":{"displayName":"Manuel Gerardo","userId":"00401288527964521163"},"user_tz":360},"id":"1bXOeDGf9GIG","outputId":"6a37bc07-d339-4e58-98ac-a009d977857c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total de registros de Amazon: (1000, 2)\n","Total de registros de IMBD: (1000, 2)\n","Total de registros de Yelp: (1000, 2)\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\irvin\\AppData\\Local\\Temp\\ipykernel_91112\\399887763.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  dfi = pd.read_csv('imdb5.txt', sep='   ', names=['review','label'], header=None, encoding='utf-8')\n"]}],"source":["dfa = pd.read_csv('amazon5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n","dfi = pd.read_csv('imdb5.txt', sep='   ', names=['review','label'], header=None, encoding='utf-8')\n","dfy = pd.read_csv('yelp5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n","\n","\n","print('Total de registros de Amazon:',dfa.shape)\n","print('Total de registros de IMBD:',dfi.shape)\n","print('Total de registros de Yelp:',dfy.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1684698561104,"user":{"displayName":"Manuel Gerardo","userId":"00401288527964521163"},"user_tz":360},"id":"nViuK0-xtV6A"},"outputs":[],"source":["with open('imdb5.txt') as f:\n","    lines = f.readlines()\n","\n","texto_limpio = []\n","\n","for linea in lines:\n","    linea_limpia = linea.strip().replace('\\t', ' ')\n","    texto_limpio.append(linea_limpia)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1684698561737,"user":{"displayName":"Manuel Gerardo","userId":"00401288527964521163"},"user_tz":360},"id":"C3lkaJs9t5Qa"},"outputs":[],"source":["data = {'review': [], 'label': []}\n","for line in texto_limpio:\n","    line = line.strip()\n","    review, label = line.rsplit(' ', 1)\n","    data['review'].append(review)\n","    data['label'].append(label)\n","\n","dfi_limpio = pd.DataFrame(data)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1684698562289,"user":{"displayName":"Manuel Gerardo","userId":"00401288527964521163"},"user_tz":360},"id":"sUUzqqqKnS9U","scrolled":true},"outputs":[],"source":["df = pd.concat([dfa, dfi, dfy], ignore_index=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684698562774,"user":{"displayName":"Manuel Gerardo","userId":"00401288527964521163"},"user_tz":360},"id":"ua23FcnrtEsv","outputId":"2f60e0fd-25d1-4d78-8535-1d32cfd61bf2"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3000 entries, 0 to 2999\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   review  3000 non-null   object\n"," 1   label   3000 non-null   int64 \n","dtypes: int64(1), object(1)\n","memory usage: 47.0+ KB\n"]}],"source":["df.info()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684698564501,"user":{"displayName":"Manuel Gerardo","userId":"00401288527964521163"},"user_tz":360},"id":"xA61QhIOtEsv","outputId":"e71efa58-52f5-412c-d009-34e441b8d76d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>So there is no way for me to plug it in here i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Good case, Excellent value.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Great for the jawbone.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Tied to charger for conversations lasting more...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The mic is great.</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  label\n","0  So there is no way for me to plug it in here i...      0\n","1                        Good case, Excellent value.      1\n","2                             Great for the jawbone.      1\n","3  Tied to charger for conversations lasting more...      0\n","4                                  The mic is great.      1"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1684698565594,"user":{"displayName":"Manuel Gerardo","userId":"00401288527964521163"},"user_tz":360},"id":"B3o2x4vItEsv","outputId":"d6bbcb77-dccc-48f1-ea4a-c2a9c7c73372"},"outputs":[{"data":{"text/plain":["review    object\n","label      int64\n","dtype: object"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.dtypes"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":135,"status":"ok","timestamp":1684698566245,"user":{"displayName":"Manuel Gerardo","userId":"00401288527964521163"},"user_tz":360},"id":"qbqC_z48wHfS"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                 review  label\n","0     So there is no way for me to plug it in here i...      0\n","1                           Good case, Excellent value.      1\n","2                                Great for the jawbone.      1\n","3     Tied to charger for conversations lasting more...      0\n","4                                     The mic is great.      1\n","...                                                 ...    ...\n","2995  I think food should have flavor and texture an...      0\n","2996                           Appetite instantly gone.      0\n","2997  Overall I was not impressed and would not go b...      0\n","2998  The whole experience was underwhelming, and I ...      0\n","2999  Then, as if I hadn't wasted enough of my life ...      0\n","\n","[3000 rows x 2 columns]\n"]}],"source":["# df[1000:1030]\n","print (df)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MfZZ0stLmWJN"},"source":["## **Pregunta - 2:**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FrkzwdjytEsw"},"source":["Realiza de nuevo un proceso de limpieza. Aplica el preprocesamiento que consideres adecuado, sin\n","embargo, deberás aplicar necesariamente alguna de las técnicas de lematización. Como aplicaremos\n","modelos embebidos pre-entrenados, queremos palabras lo más cercanas a las existentes en un\n","idioma, inglés en este caso. Aplica y justifica cualquier otro proceso de limpieza que consideres\n","adecuado. Recuerda que en esta actividad se usarán vectores embebidos para un problema de\n","clasificación, por lo que deberás tomar de acuerdo a este contexto. Justifica todas las\n","transformaciones que se apliquen."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"QHfqsUwPtEsw"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\irvin\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download('stopwords') \n","\n","mystopwords = [x for x in stopwords.words('english')]\n","\n","X = df.review\n","\n","def clean_tok(doc):\n","  #carateres alfabéticos, primero remplazamos signos de puntuación por espacios\n","  #luego removemos espacios adicionales\n","  tmpdoc = re.sub(r'[^\\w\\s]', ' ', doc)\n","  tmpdoc = re.sub(r'[\\s{2,}]', ' ', tmpdoc)\n","\n","  #Minusculas\n","  tmpdoc = tmpdoc.lower()\n","\n","  #Tokenizacion\n","  tokens = []\n","  tokens = re.findall(r'\\b[a-z]+\\b', tmpdoc)\n","\n","  #Longitud mayor a 1\n","  tokens = [i for i in tokens if len(i)>1]\n","\n","  #Stopwords\n","  tokens = [i for i in tokens if i not in mystopwords]\n","  return tokens\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["Xcleantok = [clean_tok(x) for x in X] "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\irvin\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","def clean_doc(doc):\n","  \n","  # Primero vamos a lematizar\n","  lemmatizer = WordNetLemmatizer()\n","  tmptokens = []\n","  for x in doc:\n","    tmptokens.append(lemmatizer.lemmatize(x))\n","\n","  # Y también vamos a remover tokens duplicados\n","  tokens = []\n","  [tokens.append(x) for x in tmptokens if x not in tokens]\n","\n","  return tokens"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["Xclean = [clean_doc(x) for x in Xcleantok]  "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qzKiZYuhoHor"},"source":["## **Pregunta - 3:**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1-wkYmWitEsw"},"source":["Llamar Xclean a los comentarios procesados y Y a las etiquetas. Realicemos una partición aleatoria\n","con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de\n","esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba,\n","respectivamente. Verifica que obtienes 2100 registros de entrenamiento y 450 para cada uno de\n","validación y prueba."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"RXowZ4RStEsx"},"outputs":[],"source":["Y = df.label\n","\n","# verificando que tenemos la dimensiones esperadas.\n","assert Y.shape == (3000,)\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X,y Train: 2100 2100\n","X,y Val: 450 450\n","X,y Test 450 450\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=6) \n","x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=11)\n","\n","print('X,y Train:', len(x_train), len(y_train))    \n","print('X,y Val:', len(x_val), len(y_val))\n","print('X,y Test', len(x_test), len(y_test))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xcT5_EvAHIaE"},"source":["## **Pregunta - 4:**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L6EeporptEsx"},"source":["Usando el conjunto de entrenamiento genera un vocabulario que no sea mayor a 1500 palabras, ni\n","menor a 1000. ¿Por qué es importante acotar un vocabulario inferior y superiormente? ¿Por qué\n","debe usarse solamente el conjunto de entrenamiento para generar el diccionario? Con este\n","vocabulario que obtienes, filtra los conjuntos de entrenamiento, validación y prueba, de esta\n","manera todos los comentarios usarán solamente palabras válidas de acuerdo a este vocabulario.\n","Indica el tamaño del vocabulario obtenido.\n","\n","Hasta este punto básicamente has realizado transformaciones muy análogas a las de la semana\n","pasada y que son válidas para muchos de los procesos dentro del análisis de textos. En dado caso\n","comenta con tus compañeros de equipo qué diferencias has observado. Veamos ahora la diferencia\n","con respecto a las matrices Tf-idf que aplicaste la semana pasada, con respecto a los vectores preentrenados\n","embebidos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3ax_0N8tEsx"},"outputs":[],"source":["None"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IXc4Z2m6WMUq"},"source":["## **Pregunta - 5:**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Z2XsYShltEsx"},"source":["Utilizarás los vectores embebidos FastText preentrenados por Facebook.\n","a. Incluye una tabla comparativa de pros y contras entre los modelos FastText, word2vec de\n","Google y Glove de Stanford. Puedes consultar sus páginas correspondientes:\n","https://fasttext.cc/\n","https://code.google.com/archive/p/word2vec/\n","https://nlp.stanford.edu/projects/glove/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzcEsRHitEsy"},"outputs":[],"source":["None"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1iurTQ-X5WdC"},"source":["## **Pregunta - 6:**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kzMcvpmNtEsy"},"source":["Utiliza el modelo FastText de vectores embebidos pre-entrenados de dimensión 300 para generar\n","un nuevo diccionario clave-valor, donde la “clave” será cada token o palabra de tu vocabulario y el\n","“valor” será su vector embebido de dimensión 300. Este diccionario deberá ser del mismo tamaño\n","que el vocabulario previo que hayas construido previamente.\n","https://fasttext.cc/docs/en/crawl-vectors.html\n","\n","NOTA: Debido a la cantidad de recursos computacionales que demanda cargar los vectores\n","FastText (son 2 millones de vectores), es recomendable que una vez que generes el nuevo\n","vocabulario de vectores embebidos, guardes dicho diccionario en un archivo (pickle, npz o el que\n","consideres más adecuado). Una vez realizado lo anterior, puedes borrar la variable de FastText\n","para liberar memoria RAM. De esta manera, ya tienes tu vocabulario de vectores embebidos de\n","acuerdo a los tokens que consideras más adecuados para tu problema y puedes usarlo rápidamente\n","cuando lo necesites. En dado caso apóyense entre los miembros del equipo de tener dificultades\n","para generar el vocabulario y por mientras puedes usar el archivo del vocabulario que alguno haya\n","generado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B390LyO8tEsy"},"outputs":[],"source":["None"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"m1GlIF47I15k"},"source":["## **Pregunta - 7:**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EuzoYbJGtEsy"},"source":["Una manera de utilizar los vectores embebidos con modelos de aprendizaje automático en\n","documentos de texto, es asignar a cada comentario filtrado el vector embebido de dimensión 300\n","que resulta de promediar todos sus tokens. Así, en este ejercicio deberás generar los arreglos\n","correspondientes para los conjuntos de entrenamiento, validación y prueba. Los llamaremos\n","trainEmb, valEmb y testEmb, respectivamente. ¿Cuáles son sus dimensiones? ¿Se podrían usar para\n","su representación matrices dispersas (sparse matrices) como en el caso de la matriz Tf-idf?\n","Responde a dichas preguntas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SX5saEpBtEsy"},"outputs":[],"source":["None"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L-S6wtXrXyJI"},"source":["## **Pregunta - 8:**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kT3smyYWtEsz"},"source":["Utiliza los modelos de regresión lineal y bosque aleatorio (random forest) y encuentra sus\n","desempeños (accuracy). Compara los resultados con los de la semana anterior."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iEMVTIKttEsz"},"outputs":[],"source":["None"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bZGkvGrNtEsz"},"source":["## **Pregunta - 9:**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sDixa95NtEsz"},"source":["Obtener la matriz de confusión e interpretar sus valores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlMWeolRtEsz"},"outputs":[],"source":["None"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dfMzR65Hhs-h"},"source":["## **Pregunta - 10:**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Azuzs4fStEtF"},"source":["Comenta con tus compañeros de equipo los pasos realizados en esta actividad e incluyan sus\n","conclusiones finales."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1WjY1OrjtEtG"},"outputs":[],"source":["None"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IDvRXhA_rdBy"},"source":["# **FIN DE LA ACTIVIDAD DE LA SEMANA 5**"]}],"metadata":{"colab":{"collapsed_sections":["qzKiZYuhoHor","xcT5_EvAHIaE","IXc4Z2m6WMUq","1iurTQ-X5WdC","m1GlIF47I15k","L-S6wtXrXyJI","dfMzR65Hhs-h"],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
